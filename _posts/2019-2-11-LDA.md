---
published: true
title: Linear Discriminant Analysis
collection: st
layout: single
author_profile: false
read_time: true
categories: [machinelearning]
excerpt : "Supervised Learning Algorithms"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

Linear Discriminant Analysis is a generative model for classification. It is a generalization of Fisher's linear discriminant. LDA works on continuous variables. If the classification task includes categorical variables, the equivalent technique is called the discriminant correspondance analysis.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Key ideas

- Generative Model that tries to estimate $$ P (X = x \mid Y = 1) $$ and $$ P (X = x \mid Y = -1) $$
- Used for classification
- Requires continuous variables
- Relies on normality assumption for $$ P(X \mid Y = 1) $$ and $$ P(X \mid Y = 0) $$
- Requires homoscedasticity and full rank covariances

## Theory

Let's suppose that we have 2 classes. Using LDA, we would like to find the two underlying marginal distributions. 

We'll define the distributions of the 2 classes :
- $$ G = L(X \mid Y=1) $$ distribution of the class 1
- $$ H = L(X \mid Y=-1) $$ distribution of the class -1

To understand the intuition behind LDA, we can define a likelihood ratio :
$$ \phi(X) = \frac { \delta G} { \delta H} (x) = \frac {P(X = x \mid Y = 1)} {P(X = x \mid Y = -1)} $$

Using Bayes' theorem :

$$ \phi(X) = \frac {P(Y = 1 \mid X = x) \frac {P(X=x)} {P(Y=1)}} {P(Y = -1 \mid X = x) \frac {P(X=x)} {P(Y=-1)}} $$

$$ \phi(X) = \frac { \frac { P(Y = 1 \mid X = x) } { P(Y=1) } } { \frac { P(Y = -1 \mid X = x) } { P(Y=-1) } } $$

We can re-define $$ P(Y=1) $$ as $$ p $$ and $$ P(Y=1 \mid X = x) $$ as the prior probability $$ \eta(x) $$.

$$ \phi(X) = \frac {1-p} {p} \frac {\eta(x)} {1-\eta(x)} $$

We can easily isolate the prior probability $$ \eta(x) $$ :

$$ \eta(x) = \frac { p \phi(x) } {(1-p) + p \phi(x)} $$ 

## Hypothesis

The LDA relies on some strong hypothesis which we'll explicit now.

### Gaussian marginal distributions 

- $$ G = N(\mu_+, \sigma_+) $$
- $$ H = N(\mu_-, \sigma_-) $$

where :

$$ N(\mu, \sigma^2) = \frac {1} {\sqrt {2  \pi \sigma^2}}  e^{ \frac {-1} {2 \sigma^2} {(x-\mu)^2}} $$

### Homoscedasticity

LDA should be used when the covariance matrices are equal among the 2 classes :

$$ \sigma_+ = \sigma_- = \sigma $$


## Computation

How do we find the parameters of the model ? How does the lerning process work ?


$$ \eta(x) = \frac { e^{ ( \frac {-1} {2} ( x - \mu_+ )^T \sigma^{-1} (x-\mu_+) ) } } {e^{ ( \frac {-1} {2} (x - \mu_-)^T \sigma^{-1} (x- \mu_- ) ) } } $$

$$ = e^{ ( \frac {-1} {2} (x-\mu_+)^T \sigma^{-1} (x-\mu_+) + \frac {-1} {2} (x-\mu_-)^T \sigma^{-1} (x-\mu_-) ) } $$

$$ = e^{ (x^T \sigma^{-1} {\mu_+}^T - \frac {1} {2} \mu_+ \sigma_{-1} \mu_- - x^T \sigma^{-1} \mu_- + \frac {1} {2} {\mu_-}^T \sigma_{-1} \mu_- ) } $$

If $$ \eta(x) > \frac {1} {2} $$, then $$ \phi(x) ≥ \frac {1-p} {p} $$ . This means that :

$$ x^T \sigma^{-1} (\mu_+ - \mu_-) + \frac {1} {2} ( {\mu_+}^T \sigma_{-1} \mu_- {\mu_+}^T \sigma^{-1} \mu_+) ≥ log \frac {p} {1-p} $$

Which can be re-written as :

$$ \alpha + \beta^T x ≥ 0 $$

Where 

![image](https://maelfabien.github.io/assets/images/bayes.png)
