---
published: true
title: Normal Regression Model
collection: st
layout: single
author_profile: false
read_time: true
categories: [statistics]
excerpt : "Linear Model"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

We have covered the most general models in the linear regression framework so far. It is now time to explore some special cases, and the statistical notions that support them. We will start with the normal regression model.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Concept

Recall that in a multi-dimensional linear regression, we have :  $$ Y =  X {\beta}+ {\epsilon} $$

And the following conditions on $$ {\epsilon} $$ :
- $$ E({\epsilon}) = 0 $$ , i.e a white noise condition
- $$ {\epsilon}_i ∼ iid  {\epsilon} $$ for all i = 1,...,n, i.e a homoskedasticity condition

Notice also that we never specified any distribution for $$ {\epsilon} $$. This is where the Normal law comes in. This time, we make the hypothesis that : $$ {\epsilon}_i ∼ iid  N(0, {\sigma}^2) $$. 


Independence implies that :
- $$ E({\epsilon}_i|X_i) = E({\epsilon}) = 0 $$
- $$ Var({\epsilon}_i|X_i) = {\sigma}^2 $$

A joint normality is not required in the normal regression model. We simply want to have that the conditional distribution of $$ Y $$ given $$ X $$ is normal.

## Maximum Likelihood Estimation (MLE)

First, we should observe that the previous condition on the conditional distribution of $$ Y $$ can be translated into :

$$ f(y|X) = \frac {1} {(2{\pi}{\sigma}^2)^{1/2}} e^{- \frac {1} {2{\sigma}^2} (Y - X {\beta})^2} $$

Under the assumption that the observations are independent, the conditional density becomes :
$$ f(y_1, y_2, ... | x_1, x_2,...) = \prod {f(y_i | x_i)} $$

$$ = \prod {\frac {1} {(2{\pi}{\sigma}^2)^{1/2}} e^{- \frac {1} {2{\sigma}^2} (y_i - x_i {\beta})^2} } $$

$$ = \frac {1} {(2{\pi}{\sigma}^2)^{n/2}} e^{- \frac {1} {2{\sigma}^2} \sum (y_i - x_i {\beta})^2} $$

$$ = L({\beta}, {\sigma}^2) $$

$$ L $$ is called the likelihood function. Our aim is to find the Maximum Likelihood Estimation (MLE), i.e the values of $$ {\beta} $$ such that the likelihood function is maximal. A natural interpretation is to identify the values of $$ {\beta}, {\sigma}^2 $$ that are the most likely. We can sum up the maximization problem as follows :

$$ ( \hat{\beta}, \hat{\sigma}^2 ) = {argmax}_{ ({\beta}, {\sigma}^2) }  L( {\beta}, {\sigma}^2 ) $$

You might have noticed that it is not always simple to work with products in a likelihood function. Therefore, we introduce the log-likelihood function : 

$$ l({\beta}, {\sigma}^2) = log L({\beta}, {\sigma}^2) $$

$$ = log f(y_1, y_2, ... | x_1, x_2,...) $$

$$ = - \frac {n} {2} log (2{\pi}{\sigma}^2) - \frac {1} {2{\sigma}^2} \sum (y_i - x_i {\beta})^2 $$

The maximization problem can be re-expressed as :

$$ (\hat{\beta}, \hat{\sigma}^2) = {argmax}_{( {\beta}, {\sigma}^2 )}  log L({\beta}, {\sigma}^2) $$

## First Order conditions

The MLE is usually identified numerically. In our case, we can explore it algebraically, by identifying $$ \hat{\beta}, \hat{\sigma}^2 $$ that jointly solve the First Order Conditions :

$$ (1) : \frac {d} {d {\beta}} log L({\beta}, {\sigma}^2) = 0 $$

$$ (2) : \frac {d} {d {\sigma}} log L({\beta}, {\sigma}^2) = 0 $$

It can be pretty easily shown that the MLE will give us the same results as the OLS procedure. Indeed, $$ \hat{\beta} = {(X^TX)^{-1}X^TY} = \hat{\beta_{OLS}} $$, and $$ $$ \hat{\sigma}^2 = \frac {1} {n} \sum (y_i - x_i {\beta})^2 = \hat{\sigma}_{OLS}}^2 $$

The last step is to plug-in the estimators in the initial problem :

$ l({\beta}, {\sigma}^2) = - \frac {n} {2} log (2{\pi}{\sigma}^2) - \frac {1} {2{\sigma}^2} \sum (y_i - x_i {\beta})^2 $$

$$ = - \frac {n} {2} log  (2{\pi}\hat{\sigma}^2) - \frac {n} {2} $$


> **Conclusion** : We have  covered a pretty important notion in this topic. MLEs are widely used, especially in Actuarial Science. 


