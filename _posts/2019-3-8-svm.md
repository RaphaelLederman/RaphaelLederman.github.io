---
published: true
title: Support Vector Machine
collection: articles
layout: single
author_profile: false
read_time: true
categories: [articles]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://raphaellederman.github.io/assets/images/night.jpg"
    teaser_image: "https://raphaellederman.github.io/assets/images/night.jpg"
toc: true
toc_sticky: true
---

In this article, we will briefly go through another well known algorithm, very popular and performant in the context of text classification : Support Vector Machine.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

This method does not focus on probabilities, but aims at creating a discriminant function $$f:X \xrightarrow{}y$$. The intuition of SVM in the linearly separable case is to put a line in the middle of two classes, so that the distance to the nearest positive or negative instance is maximized. It is important to note that this ignores the class distribution $$P(X|y)$$.
The SVM discriminant function has the form :

$$f(X) = w^\intercal T x + b$$

The classification rule is $$sign(f(X))$$, and the linear decision boundary is specified by $$f(x) = 0$$. If $$f$$ separates the data, the geometric distance between a point x and the decision boundary is $$\frac{yf(X)}{\|w\|}$$.

Given training data, the goal is to find a decision boundary $$w$$, $$b$$ that maximizes the geometric distance of the closest point. The optimization objective is therefore : 

$$\underset{w, b}{\text{max}} \overset{n}{\underset{i=1}{\;\text{min}}} \frac{y_i(w^\intercal T x_i + b)}{\|w\|}$$

This optimization objective can be re-written with an additional constraint, considering the fact that the objective is the same for $$k\hat{w}$$, $$k\hat{b}$$ for any non-zero scalng factor $$k$$ :

$$ \underset{w, b}{\text{min}} \frac{1}{2} {\|w\|}^2 $$

$$ \text{subject to}\; y_i(w^\intercal T x_i + b) \geq 1, \; i = 1, \ldots, n.$$

In the case where we don't make any assumption the linear separability of the training data, we relax the constraints by making the inequalities easier to satisfy. This is done with slack variables $$\xi_i \geq 0$$, one for each constraint. The sum of $$\xi_i$$ is penalized in order to avoid points being on the wrong side of the decision boundary while still satisfying the constraint with large $$\xi_i$$. The new problem can in this case be expressed as follows : 

$$ \underset{w, b, \xi}{\text{min}} \frac{1}{2} {\|w\|}^2 + C \sum_{i=1}^{n}\xi_i$$

$$ \text{subject to}\; y_i(w^\intercal T x_i + b) \geq 1 - \xi_i, \; i = 1, \ldots, n,\;\xi_i\geq 0.$$

Solving this objective leads to the dot product $$x_i \intercal T x_j$$, which allows SVM to be kernelized (using what is usually called the $$\textit{kernel trick})$$, but we won't give much more details on the resolution of the equations.
