---
published: true
title: Regularization techniques
collection: dl
layout: single
author_profile: false
read_time: true
categories: [deeplearning]
excerpt : "Deep Neural Networks"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
---

In this short article, we arre going to cover the concepts of the main regularization techniques in deep learning. We will explore those techniques in more detailed examples in further articles.

{% highlight python %}
{% endhighlight %}

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

One of the major challenges of deep learning is avoiding overfitting. Therefore, regularization offers a range of techniques to limit overfitting. They include :
- Weight decay
- Drop-out
- Batch normalization (BN)
- Data Augmentation

Overfitting can be graphically observed when your training accuracy keeps increasing while your validation/test accuracy does not increase anymore. 

The **weight decay** is a way of implementing an L2-regularization term. 

$$ w_d ‚Üê w_d ( 1 - \frac {\alpha \lambda} {m}) - \alpha  \frac {\partial L(y_i, h(x_i))} {\partial w_d} $$

where $$ \alpha $$ is the learning rate and $$ \lambda $$ the L2-regularization term. The L2-regularization penalizes large coefficients and therefore avoids overfitting.

The **drop-out** technique allows us for each neuron, during training, to randomly turn-off a connection with a given probability. This prevents co-adaptation between units. In Keras, the dropout is simply implemented this way :
```python
model.add(Dropout(0.25))
```

The **batch normalization** subtracts a measure of location, and divides by a measure of scale each input of each layer for faster learning. This reduces co-variate shift. The output of a layer $$ l-1 $$ is the input of the layer $$ l $$ . In  Keras, after the convolution layer, one can simply add :
```python
model.add(BatchNormalization())
```

**Data augmentation** is a popular way in image classification to prevent overfitting. The concept is to simply apply slight transformations on the input images (shift, scale...) to artificially increase the number of images. For example, in Keras :

```python
datagen = ImageDataGenerator(zoomrange=0.2,# randomly zoom into images
rotationrange=10,# randomly rotate images
widthshiftrange=0.1,# randomly shift images horizontally
heightshiftrange=0.1,# randomly shift images vertically
horizontalflip=True,# randomly flip images
verticalflip=False)# randomly flip images
```
