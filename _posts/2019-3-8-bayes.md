---
published: true
title: Multinomial Naïve Bayes
collection: articles
layout: single
author_profile: false
read_time: true
categories: [articles]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://raphaellederman.github.io/assets/images/night.jpg"
    teaser_image: "https://raphaellederman.github.io/assets/images/night.jpg"
toc: true
toc_sticky: true
---

In this article, we will briefly go through one of the most largely used algorithms for text classification : multinomial naïve Bayes.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

The multinomial naive bayes algorithm applies Bayes theorem : it is based on the rather strong assumption that, in the context of classification, every feature is independent of the others. This classifier will always output the category with the highest a priori probability using Bayes theorem. This algorithm has a simple and intuitive design, and is a good benchmark for classification purposes.

$$P(A|B) = \frac{P(B|A)\times P(A)}{P(B)}$$

Let’s take a concrete example to better understand the implications of this naive rule : what is the probability that the expression ”bewitching melody” is classified as ”music” ?

$$P(music|bewitching\ melody) = \frac{P(bewitching\ melody|music)\times P(music)}{P(bewitching\ melody)}$$

The goal here is only to determine whether or not the sequence ”bewitching melody” can be classified as ”music” : we can therefore discard the denominator and compare the two following values.

$$P(bewitching\ melody|music)\times P(music)$$

$$P(bewitching\ melody|not music)\times P(not\ music)$$

The problem in this case is that in order to determine what the value of the first expression is, we need to count the number of occurrences of "bewitching melody" in the sentences labelled as "music". But what if this particular expression never appears in our training corpus ? The $$\textit{a priori}$$ probability is null, leading to the value of the first expression being null as well. This is where the naive Bayes hypothesis comes in : as every word is supposed to be independent from the others, we can look at the occurrence of each word in the expression instead of the entire expression directly. The value we wish to compute can now be expressed as follows.

$$P(bewitching melody) = P(bewitching) \times P(melody)$$

$$P(bewitching melody|music) = P(bewitching|music) \times P(melody|music)$$

Here, we still have a problem : one of the words composing the sequence might not be present in the training corpus, in which case the value of the formula above will be null. An $$\textit{a priori}$$ frequency-based probability equal to zero can have the undesirable effect of wiping out all the information in the other probabilities. The solution is therefore to add some kind of smoothing, adding a correction term to every probability estimate. The most popular approach is called Laplace smoothing : given an observation $$x = (x_1, …, x_d)$$ from a multinomial distribution with N trials and parameter vector $$\Theta = (\Theta_1, …, \Theta_d)$$, the smoothed version of the data can be represented as follows.


$$\hat{\Theta_i} = \frac{x_i + \alpha}{N + \alpha \times d}    i = {1,...,d},$$

where the pseudocount $$alpha > 0$$ is the smoothing parameter ($$alpha = 0$$ corresponds to no smoothing).
