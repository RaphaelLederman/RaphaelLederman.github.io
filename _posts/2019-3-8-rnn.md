---
published: true
title: Recurrent Neural Networks and LSTM
collection: articles
layout: single
author_profile: false
read_time: true
categories: [articles]
excerpt : "Natural Language Processing"
header :
    overlay_image: "https://raphaellederman.github.io/assets/images/night.jpg"
    teaser_image: "https://raphaellederman.github.io/assets/images/night.jpg"
toc: true
toc_sticky: true
---

In this article, we will present a more complex category of models that is able capture dependencies in sequences : Recurrent Neural Networks and Long Short Term Memory cells.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

Recurrent neural networks leverage on the sequential nature of information : unlike regular neural networks where inputs are assumed to be independent of each other, these architectures progressively accumulate and capture information through the sequences. 

![image](https://raphaellederman.github.io/assets/images/rnn.jpg)

As we can see on the schema above, if the sequence we care about has a length of 5, the network would be unrolled into a 5-layer neural network, one layer for each instance. More precisely, $$x_t$$ is the input at time step $$t$$. For example, $$x_0$$ could be the embedding vector corresponding to the first word of a sentence. $$s_t$$ is the hidden state at time step $$t$$ : it corresponds to the $$\textit{memory}$$ of the network. $$s_t$$ is generally computed through a non linear function based on the previous hidden state $$s_{t-1}$$ and the input at the current step: $$s_t=f(Ux_t + Ws_{t-1})$$. $$o_t$$ is the output at step $$t$$. It could be a vector of probabilities across a corpus vocabulary if we wanted to predict the next word in a sentence (in this case for instance, $$o_t = \mathrm{softmax}(Vs_t)$$).

In such an architecture, the same set of parameters (U, V, W) are shared across all steps of the sequence, only the inputs vary : this makes the learning process faster. It is important to note that in the context of classification, whether it is emotions or personality traits detection, we might not need to produce outputs at each step : in this case, only a final vector of probabilities would be required.

As the RNN parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps : this is called Backpropagation Through Time (BPTT). This particular back-propagation phase can lead to a $$\textit{vanishing gradient}$$ phenomenon as the gradient signal can be multiplied by the weight matrix as many times as the number of steps in the sequence: in practice, these networks are limited to looking back only a few steps. If the weights in this matrix are small, the gradient signal ends up being so small that learning either slows down or stops completely. Moreover, it makes it more difficult to learn long-term dependencies in the data. Conversely, if the weights in this matrix are large, the very large gradient signal might cause learning to diverge ($$\textit{exploding gradient}$$ phenomenon).

This is one of the essential reasons why Long Short Term Memory architectures, introduced by Hochreiter & Schmidhuber [1997], have an edge over conventional feed-forward neural networks and RNN. Indeed, LSTMs have the property of selectively remembering patterns for long durations of time.
This is made possible by what is called a $$\textit{memory cell}$$. Its unique structure is composed of four main elements : an input gate, a neuron with a self-recurrent connection, a forget gate and an output gate. The self-recurrent connection ensures that the state of a memory cell can remain constant from one timestep to another. The role of the gates is to fine-tune the interactions between the memory cell and its environment using a sigmoid layer and a point-wise multiplication operation.

The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”

![image](https://raphaellederman.github.io/assets/images/lstm.jpg)

While the input gate can either allow incoming signal to alter the state of the memory cell or block it, the output gate can either allow the state of the memory cell to have an effect on other neurons or prevent it. Finally, the forget gate can influence the memory cell’s self-recurrent connection, allowing the cell to $$\textit{remember}$$ or $$\textit{forget}$$ its previous state.

Let's now describe more precisely how a layer of memory cells is updated at each step t of the sequence. For the equations, we will use the following notations :
$$x_t$$ is the input to the memory cell layer at time $$t$$; $$W_i$$, $$W_f$$, $$W_c$$, $$W_o$$, $$U_i$$, $$U_f$$, $$U_c$$, $$U_o$$ and $$V_o$$ are weight matrices; $$b_i$$, $$b_f$$, $$b_c$$ and $$b_o$$ are bias vectors.
The first equations give the value for the input gate $$i_t$$ and the candidate value for the states of the memory cells $$S_t$$ at time $$t$$ :

$$i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)$$

$$\widetilde{S_t} = tanh(W_c x_t + U_c h_{t-1} + b_c)$$

Then we compute the value for $$f_t$$, the activation function of the memory cells’ forget gates at time $$t$$:

$$f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)$$

Given the values of the input and forget gate activation functions $$i_t$$ and $$f_t$$, and the candidate state value $$\widetilde{S_t}$$, we can compute $$S_t$$ the memory cells’ new state at time $$t$$:

$$S_t = i_t  \times \widetilde{S_t} + f_t \times S_{t-1}$$

The new state of the memory cells allows us to compute their output gates values and finally their outputs:

$$o_t = \sigma(W_o x_t + U_o h_{t-1} + V_o S_t + b_o)$$

$$h_t = o_t \times tanh(C_t)$$
